{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Extracts hidden states of QWEN model given different datasets"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from Get_Go_Emo import get_go\n",
       "from Get_Isear import get_isr"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "goEmo = get_go()\n",
       "goEmo = goEmo[:32]\n",
       "goEmo"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "isear = get_isr()\n",
       "isear = isear[:32]\n",
       "isear"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "from transformers import AutoTokenizer, AutoModel\n",
       "from torch.utils.data import TensorDataset, DataLoader\n",
       "import pandas as pd\n",
       "import json\n",
       "import time\n",
       "import os\n",
       "\n",
       "def extract_hidden_states(df, model_names, text_column='clean_text', batch_size=16, dataset_name=\"no_name\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
       "    \"\"\"\n",
       "    Extracts hidden states for each text in the DataFrame using specified models.\n",
       "    \n",
       "    Args:\n",
       "        df (pd.DataFrame): Input DataFrame containing the text data.\n",
       "        model_names (list): List of model names to extract hidden states from.\n",
       "        text_column (str): Name of the column containing text data.\n",
       "        batch_size (int): Batch size for processing.\n",
       "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
       "    \n",
       "    Returns:\n",
       "        pd.DataFrame: DataFrame with added columns for each model's hidden states.\n",
       "    \"\"\"\n",
       "    os.makedirs('hidden_states', exist_ok=True)\n",
       "    \n",
       "    for model_name in model_names:\n",
       "        print(f\"\\nProcessing model: {model_name} for Dataset : {dataset_name}\")\n",
       "        model_start_time = time.time()\n",
       "\n",
       "        # Load tokenizer and model\n",
       "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "        if tokenizer.pad_token is None:\n",
       "            tokenizer.pad_token = tokenizer.eos_token\n",
       "\n",
       "        model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
       "        model.eval()\n",
       "        model.to(device)\n",
       "        \n",
       "        # Tokenize texts\n",
       "        texts = df[text_column].tolist()\n",
       "        tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
       "        \n",
       "        # Create DataLoader\n",
       "        input_ids = tokenized['input_ids']\n",
       "        attention_mask = tokenized['attention_mask']\n",
       "        dataset = TensorDataset(input_ids, attention_mask)\n",
       "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
       "        \n",
       "        all_hidden_dicts = []\n",
       "        total_batches = len(dataloader)\n",
       "        \n",
       "        with torch.no_grad():\n",
       "            for batch_idx, batch in enumerate(dataloader):\n",
       "                batch_start_time = time.time()\n",
       "                \n",
       "                input_ids_batch, attention_mask_batch = [t.to(device) for t in batch]\n",
       "                \n",
       "                outputs = model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
       "                hidden_states = outputs.hidden_states\n",
       "                \n",
       "                current_batch_size = input_ids_batch.size(0)\n",
       "                for i in range(current_batch_size):\n",
       "                    example_hidden = {}\n",
       "                    for layer_idx, layer in enumerate(hidden_states):\n",
       "                        cls_embedding = layer[i, 0, :].cpu().numpy().tolist()\n",
       "                        example_hidden[f'layer_{layer_idx}'] = cls_embedding\n",
       "                    all_hidden_dicts.append(example_hidden)\n",
       "                \n",
       "                batch_time = time.time() - batch_start_time\n",
       "                print(\n",
       "                    f\"Batch {batch_idx + 1}/{total_batches} | \"\n",
       "                    f\"Time: {batch_time:.2f}s | \"\n",
       "                    f\"Avg: {(time.time() - model_start_time)/(batch_idx + 1):.2f}s/batch\", \n",
       "                    end='\\r'\n",
       "                )\n",
       "        \n",
       "        # Save to JSON\n",
       "        output_filename = os.path.join('hidden_states', f\"{model_name}_{dataset_name}.json\")\n",
       "        with open(output_filename, 'w') as f:\n",
       "            json.dump(all_hidden_dicts, f, indent=2)\n",
       "        \n",
       "        # Print summary\n",
       "        total_time = time.time() - model_start_time\n",
       "        print(f\"\\nCompleted {model_name} in {total_time:.2f}s ({total_time/len(df):.4f}s/sample)\")\n",
       "        print(f\"Saved hidden states to {output_filename}\")\n",
       "        \n",
       "        # Cleanup\n",
       "        del model, tokenizer\n",
       "        torch.cuda.empty_cache()\n",
       "    \n",
       "    # Save complete DataFrame\n",
       "    final_output_path = os.path.join('hidden_states', f'all_hidden_states_{dataset_name}.json')\n",
       "    df.to_json(final_output_path, orient='records', indent=2)\n",
       "    print(f\"\\nSaved complete DataFrame with all hidden states to {final_output_path}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Example usage\n",
       "model_names = ['bert-base-uncased', 'gpt2']\n",
       "\n",
       "# Process goEmo dataset\n",
       "goEmo_with_hidden = extract_hidden_states(goEmo, model_names, dataset_name=\"goEmo\")\n",
       "\n",
       "# Process isear dataset\n",
       "isear_with_hidden = extract_hidden_states(isear, model_names, dataset_name=\"isear\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Add plots for each of the model names, a series of plots"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from analysis import describe_hidden_states, analyze_hidden_states, describe_all_hidden_states"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Example usage\n",
       "analysis = analyze_hidden_states()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Describe all files in the default hidden_states directory\n",
       "describe_all_hidden_states()"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }